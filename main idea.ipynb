{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import pm4py"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main event log loaded - BPI challenge 2016 - work site"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "\n",
    "file_path = r\"C:\\Users\\M\\Desktop\\thesis\\PM in costumer behavior\\codes\\BPI2016_Clicks_Logged_In.csv\\samples.csv\"\n",
    "event_log = pm4py.format_dataframe(pandas.read_csv(file_path, sep=',',encoding= 'unicode_escape'), case_id='case_id',activity_key='activity',timestamp_key='timestamp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "event_log.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def import_csv(file_path):\n",
    "    #event_log = pandas.read_csv(file_path, sep=',')\n",
    "    #event_log = pm4py.format_dataframe(event_log, case_id='case_id', activity_key='activity', timestamp_key='timestamp')\n",
    "    event_log = pandas.read_csv(file_path, sep=',',encoding= 'unicode_escape')\n",
    "    num_events = len(event_log)\n",
    "    num_cases = len(event_log.case_id.unique())\n",
    "    print(\"Number of events: {}\\nNumber of cases: {}\".format(num_events, num_cases))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import_csv(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pm4py\n",
    "\n",
    "def import_csv(file_path):\n",
    "    event_log = pandas.read_csv(file_path, sep=',')\n",
    "    event_log = pm4py.format_dataframe(event_log, case_id='case_id', activity_key='activity', timestamp_key='timestamp')\n",
    "    start_activities = pm4py.get_start_activities(event_log)\n",
    "    end_activities = pm4py.get_end_activities(event_log)\n",
    "    print(\"Start activities: {}\\nEnd activities: {}\".format(start_activities, end_activities))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import_csv(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# import XES BPI challenge 2015"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\Users\\M\\.conda\\pkgs\\graphviz-2.38-hfd603c8_2\\Library\\bin\"\n",
    "\n",
    "\n",
    "import pm4py\n",
    "log = pm4py.read_xes(r\"C:\\Users\\M\\Desktop\\thesis\\PM in costumer behavior\\codes\\BPI challenge 2015 Municipality 2\\BPIC15_2.xes\")\n",
    "\n",
    "dfg, start_activities, end_activities = pm4py.discover_dfg(log)\n",
    "#pm4py.view_dfg(dfg, start_activities, end_activities)\n",
    "#pm4py.save_vis_dfg(dfg, start_activities, end_activities, \"trial.png\")\n",
    "\n",
    "dfg #dictionary of graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#create the dataframe of pair activities for graph\n",
    "df_dfg = pd.DataFrame.from_dict(dfg, orient='index').reset_index()\n",
    "df_dfg.columns = ['pairs','frequency']\n",
    "print(df_dfg.shape)\n",
    "\n",
    "df_dfg['activity1'], df_dfg['activity2'] = zip(*df_dfg.pairs)\n",
    "df_dfg.head(5)\n",
    "#activities_count = pm4py.get_attribute_values(log, \"concept:name\")\n",
    "#activities_count\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create network nodes: each activity, edges: frequency of traces as a weight of each edges\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "G1 = nx.from_pandas_edgelist(df_dfg,\n",
    "                            source='activity1',\n",
    "                            target='activity2',\n",
    "                            edge_attr='frequency',\n",
    "                            create_using=nx.DiGraph())\n",
    "\n",
    "print(nx.info(G1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "H = G1.to_undirected()\n",
    "centrality= betCent\n",
    "edges,weights = zip(*nx.get_edge_attributes(H,'frequency').items())\n",
    "\n",
    "\n",
    "# compute community structure\n",
    "lpc = nx.community.label_propagation_communities(H)\n",
    "community_index = {n: i for i, com in enumerate(lpc) for n in com}\n",
    "\n",
    "#### draw graph ####\n",
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "pos = nx.spring_layout(H, k=0.9, seed=4572321)\n",
    "node_color = \"b\"\n",
    "node_size = [100]\n",
    "nx.draw_networkx(\n",
    "    H,\n",
    "    pos=pos,\n",
    "    with_labels=True,\n",
    "    node_color=node_color,\n",
    "    node_size=node_size,\n",
    "    #edge_color=\"gainsboro\",\n",
    "    alpha=0.9,\n",
    "    edge_color=weights\n",
    "\n",
    ")\n",
    "\n",
    "# Title/legend\n",
    "font = {\"color\": \"k\", \"fontweight\": \"bold\", \"fontsize\": 20}\n",
    "ax.set_title(\"DFG \", font)\n",
    "# Change font color for legend\n",
    "font[\"color\"] = \"r\"\n",
    "\n",
    "\n",
    "\n",
    "# Resize figure for label readibility\n",
    "ax.margins(0.1, 0.05)\n",
    "fig.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# betweenness Centrality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G1)\n",
    "betCent = nx.betweenness_centrality(G1, normalized=True, endpoints=True)\n",
    "node_color = [20000.0 * G1.degree(v) for v in G1]\n",
    "node_size =  [v * 10000 for v in betCent.values()]\n",
    "plt.figure(figsize=(20,20))\n",
    "nx.draw_networkx(G1, pos=pos, with_labels=False, node_color=node_color, node_size=node_size )\n",
    "plt.axis('off')\n",
    "sorted(betCent,key=betCent.get,  reverse=True)[:25]\n",
    "sorted(node_size)[:25]\n",
    "#betCent\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "H = G1.to_undirected()\n",
    "centrality= betCent\n",
    "\n",
    "# compute community structure\n",
    "lpc = nx.community.label_propagation_communities(H)\n",
    "community_index = {n: i for i, com in enumerate(lpc) for n in com}\n",
    "\n",
    "#### draw graph ####\n",
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "pos = nx.spring_layout(H, k=1, seed=4572321)\n",
    "node_color = [20000.0 * G1.degree(v) for v in G1]\n",
    "#node_color = [community_index[n] for n in H]\n",
    "node_size = [v * 20000 for v in centrality.values()]\n",
    "nx.draw_networkx(\n",
    "    H,\n",
    "    pos=pos,\n",
    "    with_labels=False,\n",
    "    node_color=node_color,\n",
    "    node_size=node_size,\n",
    "    edge_color=\"gainsboro\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Title/legend\n",
    "font = {\"color\": \"k\", \"fontweight\": \"bold\", \"fontsize\": 20}\n",
    "ax.set_title(\"Betweenness centrality \", font)\n",
    "# Change font color for legend\n",
    "font[\"color\"] = \"r\"\n",
    "\n",
    "\n",
    "\n",
    "# Resize figure for label readibility\n",
    "ax.margins(0.1, 0.05)\n",
    "fig.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "between = betCent\n",
    "{k:between[k] for k in between if between[k]==max(between.values())}\n",
    "betweenness =sorted(betCent, key=betCent.get, reverse=True)[:25]\n",
    "\n",
    "betweenness={\"betweenness page\":betweenness}\n",
    "df_betweenness = pd.DataFrame(betweenness)\n",
    "df_betweenness.to_excel(\"betweenness.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Degree Centrality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G1, k=1)\n",
    "degCent = nx.degree_centrality(G1)\n",
    "degCent =dict(G1.in_degree(weight='edge_attr'))\n",
    "node_color = [20000.0 * G1.degree(v) for v in G1]\n",
    "node_size =  [v * 10000 for v in degCent.values()]\n",
    "plt.figure(figsize=(150,150))\n",
    "nx.draw_networkx(G1, pos=pos, with_labels=False,\n",
    "                 node_color=node_color,\n",
    "                 node_size=node_size,\n",
    "                edge_color=\"gray\",\n",
    "                #alpha=0.4,\n",
    "                )\n",
    "plt.axis('off')\n",
    "\n",
    "deg_cen_points = degCent\n",
    "{k:deg_cen_points[k] for k in deg_cen_points if deg_cen_points[k]==max(deg_cen_points.values())}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "degree = sorted(degCent, key=degCent.get, reverse=True)[:25]\n",
    "\n",
    "degree={\"degree page\":degree}\n",
    "df_degree = pd.DataFrame(degree)\n",
    "df_degree.to_excel(\"degree.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted(degCent, key=degCent.get, reverse=True)[:25]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Closeness Centrality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G1, k=1)\n",
    "cloCent = nx.closeness_centrality(G1)\n",
    "node_color = [20000.0 * G1.degree(v) for v in G1]\n",
    "node_size =  [v * 10000 for v in cloCent.values()]\n",
    "plt.figure(figsize=(130,130))\n",
    "nx.draw_networkx(G1, pos=pos, with_labels=False,\n",
    "                 node_color=node_color,\n",
    "                node_size=node_size )\n",
    "plt.axis('off')\n",
    "\n",
    "cloCent_points = cloCent\n",
    "{k:cloCent_points[k] for k in cloCent_points if cloCent_points[k]==max(cloCent_points.values())}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "closeness = sorted(cloCent, key=cloCent.get, reverse=True)[:25]\n",
    "closeness={\"closeness page\":closeness}\n",
    "df_closeness = pd.DataFrame(closeness)\n",
    "df_closeness.to_excel(\"closeness.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# page rank Centrality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "page_rank = dict(nx.pagerank(G1,weight='edge_attr'))\n",
    "{k:page_rank[k] for k in page_rank if page_rank[k]==max(page_rank.values())}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G1)\n",
    "pageRankCent = nx.pagerank(G1,weight='edge_attr')\n",
    "node_color = [20000.0 * G1.degree(v) for v in G1]\n",
    "node_size =  [v * 10000 for v in pageRankCent.values()]\n",
    "plt.figure(figsize=(90,90))\n",
    "nx.draw_networkx(G1, pos=pos, with_labels=False,\n",
    "                 node_color=node_color,\n",
    "                node_size=node_size )\n",
    "plt.axis('off')\n",
    "\n",
    "pageRankCent_points = pageRankCent\n",
    "{k:pageRankCent_points[k] for k in pageRankCent_points if pageRankCent_points[k]==max(pageRankCent_points.values())}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "H = G1\n",
    "centrality= pageRankCent\n",
    "\n",
    "# compute community structure\n",
    "#lpc = nx.community.label_propagation_communities(H)\n",
    "#community_index = {n: i for i, com in enumerate(lpc) for n in com}\n",
    "\n",
    "#### draw graph ####\n",
    "fig, ax = plt.subplots(figsize=(40, 30))\n",
    "pos = nx.spring_layout(H, k=0.7, seed=4572321)\n",
    "node_color = [20000.0 * G1.degree(v) for v in G1]\n",
    "node_size =  [v * 10000 for v in eigCent.values()]\n",
    "nx.draw_networkx(\n",
    "    H,\n",
    "    pos=pos,\n",
    "    with_labels=False,\n",
    "    node_color=node_color,\n",
    "    node_size=node_size,\n",
    "    edge_color=\"grey\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Title/legend\n",
    "font = {\"color\": \"k\", \"fontweight\": \"bold\", \"fontsize\": 20}\n",
    "ax.set_title(\" Pagerank Centrality \", font)\n",
    "# Change font color for legend\n",
    "font[\"color\"] = \"r\"\n",
    "\n",
    "\n",
    "\n",
    "# Resize figure for label readibility\n",
    "ax.margins(0.1, 0.05)\n",
    "fig.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pageRank = sorted(page_rank, key=page_rank.get, reverse=True)[:25]\n",
    "pageRank={\"page_rank page\":pageRank}\n",
    "df_pageRank = pd.DataFrame(pageRank)\n",
    "df_pageRank.to_excel(\"pageRank.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Degree Graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max(x for x,y in nx.degree(G1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Eeigenvector Centrality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G1, k=10)\n",
    "eigCent = nx.eigenvector_centrality(G1)\n",
    "node_color = [20000.0 * G1.degree(v) for v in G1]\n",
    "node_size =  [v * 100 for v in eigCent.values()]\n",
    "plt.figure(figsize=(10,10))\n",
    "nx.draw_networkx(G1, with_labels=False,\n",
    "                 node_color=node_color,\n",
    "                 node_size=node_size )\n",
    "plt.axis('off')\n",
    "\n",
    "eigCent_points = eigCent\n",
    "{k:eigCent_points[k] for k in eigCent_points if eigCent_points[k]==max(eigCent_points.values())}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "H = G1\n",
    "centrality= eigCent\n",
    "\n",
    "# compute community structure\n",
    "#lpc = nx.community.label_propagation_communities(H)\n",
    "#community_index = {n: i for i, com in enumerate(lpc) for n in com}\n",
    "\n",
    "#### draw graph ####\n",
    "fig, ax = plt.subplots(figsize=(40, 40))\n",
    "pos = nx.spring_layout(H, k=1, seed=4572321)\n",
    "node_color = [20000.0 * G1.degree(v) for v in G1]\n",
    "node_size =  [v * 10000 for v in eigCent.values()]\n",
    "nx.draw_networkx(\n",
    "    H,\n",
    "    pos=pos,\n",
    "    with_labels=False,\n",
    "    node_color=node_color,\n",
    "    node_size=node_size,\n",
    "    edge_color=\"grey\",\n",
    "    alpha=1,\n",
    ")\n",
    "\n",
    "# Title/legend\n",
    "font = {\"color\": \"k\", \"fontweight\": \"bold\", \"fontsize\": 20}\n",
    "ax.set_title(\" Eigenvector Centrality \", font)\n",
    "# Change font color for legend\n",
    "font[\"color\"] = \"r\"\n",
    "\n",
    "\n",
    "\n",
    "# Resize figure for label readibility\n",
    "ax.margins(0.1, 0.05)\n",
    "fig.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eigenvalue=sorted(eigCent, key=eigCent.get, reverse=True)[:600]\n",
    "\n",
    "#eigenvalue={\"eigenvalue score\":eigCent}\n",
    "df_eigenvalue = pd.DataFrame(eigenvalue)\n",
    "df_eigenvalue.to_excel(\"eigenvalue.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "density = nx.density(G1)\n",
    "print(density)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# merge centralities\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfALL = pd.DataFrame(list(eigCent.items()),columns=['name','eigCent'])\n",
    "dfALL['cloCent']=dfALL['name'].map(cloCent)\n",
    "dfALL['betCent']=dfALL['name'].map(betCent)\n",
    "dfALL['degCent']=dfALL['name'].map(degCent)\n",
    "dfALL['pageRankCent']=dfALL['name'].map(pageRankCent)\n",
    "dfALL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "degree = G1.degree()\n",
    "degree_list = []\n",
    "\n",
    "for (n,d) in degree:\n",
    "    degree_list.append(d)\n",
    "\n",
    "av_degree = sum(degree_list) / len(degree_list)\n",
    "\n",
    "print('The average degree is ' + str(av_degree))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(degree_list,label='Degree Distribution')\n",
    "plt.axvline(av_degree,color='r',linestyle='dashed',label='Average Degree')\n",
    "plt.legend()\n",
    "plt.ylabel('Number of Nodes')\n",
    "plt.title('DFG: Node Degree')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_palette(sns.color_palette(\"pastel\"))\n",
    "ax=sns.violinplot(degree_list,color='0.6',orient='h',palette ='plasma')\n",
    "# Customize the axes and title\n",
    "ax.set_title(\"BPI challenge 2015 dataset: Degree ditribution\")\n",
    "ax.set_xlabel(\"Degree\")\n",
    "ax.set_ylabel(\"Number of Nodes\")\n",
    "#ax=sns.swarmplot(degree_list)\n",
    "image = ax\n",
    "image.figure.savefig('ax.png', format='png', dpi=1200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clustring algorithms - DBSCAN , Gauusian"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train dataset and prepration\n",
    "train_data=dfALL.drop(columns=\"name\").values\n",
    "train_data\n",
    "X=train_data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#DBSCAN\n",
    "\n",
    "from numpy import where\n",
    "from numpy import unique\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "\n",
    "\n",
    "train_data=train_data=dfALL.drop(columns=\"name\").values\n",
    "\n",
    "# Define model\n",
    "dbscan_model = DBSCAN(eps=0.2, min_samples=4)\n",
    "\n",
    "# Train the model\n",
    "dbscan_model.fit(train_data)\n",
    "\n",
    "# Assign each data point to a cluster\n",
    "dbscan_res = dbscan_model.fit_predict(train_data)\n",
    "\n",
    "# obtain all the unique clusters\n",
    "dbscan_clstrs = unique(dbscan_res)\n",
    "\n",
    "# Plot the DBSCAN clusters\n",
    "for dbscan_clstr in dbscan_clstrs:\n",
    "    # Obtain data point that belong in this cluster\n",
    "    index = where(dbscan_res == dbscan_clstr)\n",
    "    # plot\n",
    "    plot.scatter(train_data[index, 0], train_data[index, 1])\n",
    "\n",
    "# show plot\n",
    "plot.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "fig = plt.figure(figsize=(80, 80))\n",
    "fig.subplots_adjust(hspace=.5, wspace=.2)\n",
    "i = 1\n",
    "for x in range(10, 0, -1):\n",
    "    eps = 1/(11-x)\n",
    "    db = DBSCAN(eps=eps, min_samples=2).fit(X)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "\n",
    "\n",
    "    print(eps)\n",
    "    ax = fig.add_subplot(5, 2, i)\n",
    "    #ax.text(1, 4, \"eps = {}\".format(round(eps, 1)), fontsize=25, ha=\"center\")\n",
    "    sns.scatterplot(X[:,0], X[:,1], hue=[\"cluster-{}\".format(x) for x in labels])\n",
    "\n",
    "    i += 1\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig.savefig(\"eps comparison.png\", dpi = 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Finding best values of eps and min_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Defining the list of hyperparameters to try\n",
    "eps_list=np.arange(start=0.1, stop=0.9, step=0.01)\n",
    "min_sample_list=np.arange(start=2, stop=12, step=1)\n",
    "\n",
    "# Creating empty data frame to store the silhouette scores for each trials\n",
    "silhouette_scores_data=pd.DataFrame()\n",
    "\n",
    "for eps_trial in eps_list:\n",
    "    for min_sample_trial in min_sample_list:\n",
    "\n",
    "        # Generating DBSAN clusters\n",
    "        db = DBSCAN(eps=eps_trial, min_samples=min_sample_trial)\n",
    "\n",
    "        if(len(np.unique(db.fit_predict(X)))>=1):\n",
    "            sil_score=silhouette_score(X, db.fit_predict(X))\n",
    "        else:\n",
    "            continue\n",
    "        trial_parameters=\"eps:\" + str(eps_trial.round(1)) +\" min_sample :\" + str(min_sample_trial)\n",
    "\n",
    "        silhouette_scores_data=silhouette_scores_data.append(pd.DataFrame(data=[[sil_score,trial_parameters]], columns=[\"score\", \"parameters\"]))\n",
    "\n",
    "# Finding out the best hyperparameters with highest Score\n",
    "silhouette_scores_data.sort_values(by='score', ascending=False).head(200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# DBSCAN Clustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=0.4, min_samples=2)\n",
    "# Plotting the clusters\n",
    "plt.scatter(x= X[:,0], y= X[:,1], c=db.fit_predict(X))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dbscan_cluster = DBSCAN(eps=0.4, min_samples=2)\n",
    "dbscan_cluster.fit(X)\n",
    "\n",
    "# Visualizing DBSCAN\n",
    "plt.scatter(X[:, 0],\n",
    "X[:, 1],\n",
    "c=dbscan_cluster.labels_)#, label=y)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "# Number of Clusters\n",
    "labels=dbscan_cluster.labels_\n",
    "N_clus=len(set(labels))-(1 if -1 in labels else 0)\n",
    "print('Estimated no. of clusters: %d' % N_clus)\n",
    "\n",
    "# Identify Noise\n",
    "n_noise = list(dbscan_cluster.labels_).count(-1)\n",
    "print('Estimated no. of noise points: %d' % n_noise)\n",
    "\n",
    "# Calculating v_measure\n",
    "#print('v_measure =', v_measure_score(y, labels))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "\n",
    "X= train_data\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.2, min_samples=9).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "#print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "#print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "#print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "#print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(labels_true, labels))\n",
    "#print(\n",
    "#    \"Adjusted Mutual Information: %0.3f\"\n",
    "#    % metrics.adjusted_mutual_info_score(labels_true, labels)\n",
    "#)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, labels))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=14,\n",
    "    )\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=6,\n",
    "    )\n",
    "\n",
    "plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Silhouette Coefficient\n",
    "The best value of the Silhouette Coefficient is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gaussian mixture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numpy import where\n",
    "from numpy import unique\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "\n",
    "gaussian_mdl = GaussianMixture(n_components=18)\n",
    "\n",
    "# model training\n",
    "gaussian_mdl.fit(train_data)\n",
    "\n",
    "# data points assigned to a cluster\n",
    "gaussian_res = gaussian_mdl.fit_predict(train_data)\n",
    "\n",
    "# get clusters which are unique\n",
    "gaussian_clstr = unique(gaussian_res)\n",
    "\n",
    "# Plot\n",
    "for gaussian_cluser in gaussian_clstr:\n",
    "\n",
    "    index = where(gaussian_res == gaussian_cluser)\n",
    "    # plot\n",
    "    plot.scatter(train_data[index, 0], train_data[index, 1])\n",
    "\n",
    "# show plot\n",
    "plot.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "\n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "\n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs))\n",
    "\n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
    "    ax.axis('equal')\n",
    "\n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covars_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=4, random_state=42).fit(X)\n",
    "plot_results(X, gmm.predict(X), gmm.means_, gmm.covariances_, 0, \"Gaussian Mixture, n_components=4\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_components = np.arange(1, 100)\n",
    "models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(X)\n",
    "          for n in n_components]\n",
    "\n",
    "plt.plot(n_components, [m.bic(X) for m in models], label='BIC')\n",
    "plt.plot(n_components, [m.aic(X) for m in models], label='AIC')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('n_components');\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gmm = mixture.GaussianMixture(n_components=44, covariance_type=\"full\")\n",
    "sil_score=silhouette_score(X, gmm.fit_predict(X))\n",
    "sil_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm.html#sphx-glr-auto-examples-mixture-plot-gmm-py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn import mixture\n",
    "\n",
    "color_iter = itertools.cycle([\"navy\", \"c\", \"cornflowerblue\", \"gold\", \"darkorange\"])\n",
    "\n",
    "\n",
    "def plot_results(X, Y_, means, covariances, index, title):\n",
    "    splot = plt.subplot(2, 1, 1 + index)\n",
    "    for i, (mean, covar, color) in enumerate(zip(means, covariances, color_iter)):\n",
    "        v, w = linalg.eigh(covar)\n",
    "        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)\n",
    "        u = w[0] / linalg.norm(w[0])\n",
    "        # as the DP will not use every component it has access to\n",
    "        # unless it needs it, we shouldn't plot the redundant\n",
    "        # components.\n",
    "        if not np.any(Y_ == i):\n",
    "            continue\n",
    "        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], 0.8, color=color)\n",
    "\n",
    "        # Plot an ellipse to show the Gaussian component\n",
    "        angle = np.arctan(u[1] / u[0])\n",
    "        angle = 180.0 * angle / np.pi  # convert to degrees\n",
    "        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180.0 + angle, color=color)\n",
    "        ell.set_clip_box(splot.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        splot.add_artist(ell)\n",
    "\n",
    "    #plt.xlim(-9.0, 5.0)\n",
    "    #plt.ylim(-3.0, 6.0)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)\n",
    "\n",
    "\n",
    "\n",
    "# Fit a Gaussian mixture with EM using 44 components\n",
    "gmm = mixture.GaussianMixture(n_components=44, covariance_type=\"full\").fit(X)\n",
    "plot_results(X, gmm.predict(X), gmm.means_, gmm.covariances_, 0, \"Gaussian Mixture, n_components=44\")\n",
    "\n",
    "# Fit a Dirichlet process Gaussian mixture using 44 components\n",
    "dpgmm = mixture.BayesianGaussianMixture(n_components=44, covariance_type=\"full\").fit(X)\n",
    "plot_results(\n",
    "    X,\n",
    "    dpgmm.predict(X),\n",
    "    dpgmm.means_,\n",
    "    dpgmm.covariances_,\n",
    "    1,\n",
    "    \"Bayesian Gaussian Mixture with a Dirichlet process prior, n_components=44\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://github.com/vlavorini/ClusterCardinality/blob/master/Cluster%20Cardinality.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.errorbar(n_clusters, sils, yerr=sils_err)\n",
    "plt.title(\"Silhouette Scores\", fontsize=20)\n",
    "plt.xticks(n_clusters)\n",
    "plt.xlabel(\"N. of clusters\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(nx.info(G1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#BPMN model\n",
    "import pm4py\n",
    "log = pm4py.read_xes(r\"C:\\Users\\M\\Desktop\\thesis\\PM in costumer behavior\\codes\\BPI challenge 2015 Municipality 2\\BPIC15_2.xes\")\n",
    "\n",
    "process_tree = pm4py.discover_tree_inductive(log)\n",
    "bpmn_model = pm4py.convert_to_bpmn(process_tree)\n",
    "pm4py.view_bpmn(bpmn_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#process tree\n",
    "process_tree = pm4py.discover_tree_inductive(log)\n",
    "pm4py.view_process_tree(process_tree)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Feature selection\n",
    "from pm4py.algo.transformation.log_to_features import algorithm as log_to_features\n",
    "\n",
    "data, feature_names = log_to_features.apply(log)\n",
    "print(feature_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_features = pd.DataFrame(data, columns=feature_names)\n",
    "print(df_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PCA – Reducing the number of features\n",
    "\n",
    "Some techniques (such as the clustering, prediction, anomaly detection) suffer if the dimensionality of the dataset is too high. Hence, a dimensionality reduction technique (as PCA) helps to cope with the complexity of the data.\n",
    "It is possible to reduce the number of features using a techniques like PCA.\n",
    "Let’s create the PCA with a number of components equal to 5, and apply the PCA to the dataframe\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "df2 = pd.DataFrame(pca.fit_transform(df_features))\n",
    "print(df2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Heuristic miner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "import os\n",
    "#log_path = os.path.join(\"tests\", \"compressed_input_data\", \"09_a32f0n00.xes.gz\")\n",
    "#log = xes_importer.apply(log_path)\n",
    "\n",
    "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
    "heu_net = heuristics_miner.apply_heu(log, parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: 0.99})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
    "net, im, fm = heuristics_miner.apply(log, parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: 0.99})\n",
    "\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "gviz = pn_visualizer.apply(net, im, fm)\n",
    "pn_visualizer.view(gviz)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# inductive miner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.visualization.process_tree import visualizer as pt_visualizer\n",
    "\n",
    "tree = inductive_miner.apply_tree(log)\n",
    "\n",
    "gviz = pt_visualizer.apply(tree)\n",
    "pt_visualizer.view(gviz)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
    "net, initial_marking, final_marking = pt_converter.apply(tree, variant=pt_converter.Variants.TO_PETRI_NET)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation Miner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
